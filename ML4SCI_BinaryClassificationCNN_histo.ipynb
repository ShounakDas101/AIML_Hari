{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShounakDas101/AIML_Hari/blob/main/ML4SCI_BinaryClassificationCNN_histo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrgT5oYocjoG",
        "outputId": "861ecb46-e6bf-400c-9790-fb0ef1bf8027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import h5py\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfDwFQPedS0Y",
        "outputId": "7a57f0f3-b252-4f4a-871a-a06323760b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f5axppRQcjoN"
      },
      "outputs": [],
      "source": [
        "DATA_DIR='./data'\n",
        "'''\n",
        "IMAGE_SIZE = 32 # W=32 H=32\n",
        "NUM_CHANNELS = 2 # R G B then it should be 3\n",
        "PATCH_SIZE = 4 # W_patch=4 H_patch=4\n",
        "NUM_PATCHES = (IMAGE_SIZE//PATCH_SIZE)*(IMAGE_SIZE//PATCH_SIZE) # = 64\n",
        "NUM_HEADS = 8  # enbedding dim should be divisible by no of heads\n",
        "EMBBEDING_DIM = PATCH_SIZE*PATCH_SIZE*NUM_CHANNELS  #NUM_PATCHNELS\n",
        "MLP_SIZE = 128\n",
        "NUM_LAYERS = 8\n",
        "'''\n",
        "NUM_CLASSES = 2\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 40\n",
        "DROP_OUT = 0.1\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6-3QpHUcjoO",
        "outputId": "0a1e0c26-461f-4c33-8cfb-eb18039ec46e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# clearing cuda cache memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAO9GsjVcjoO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import os\n",
        "\n",
        "# Specify the path to the file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "\n",
        "# CHECKPOINT_PATH = '/content/drive/MyDrive/CHECKPOINTS'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"The file '{file_path}' exists.\")\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quSZE_AicjoP"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "'''\n",
        "def find_keys(hdf5_obj, path='/'):\n",
        "    \"\"\"Recursively find keys in an HDF5 file.\"\"\"\n",
        "    keys = []\n",
        "    for key in hdf5_obj[path].keys():\n",
        "        full_path = f\"{path}/{key}\"\n",
        "        keys.append(full_path)\n",
        "        if isinstance(hdf5_obj[full_path], h5py.Group):\n",
        "            keys.extend(find_keys(hdf5_obj, full_path))\n",
        "    return keys\n",
        "\n",
        "# Open the HDF5 file\n",
        "file_path = 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'  # Replace with your actual file path\n",
        "with h5py.File(file_path, 'r') as hdf5_file:\n",
        "    # Find keys starting from the root\n",
        "    all_keys = find_keys(hdf5_file)\n",
        "\n",
        "# Print the found keys\n",
        "for key in all_keys:\n",
        "    print(key)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJfT42R9cjoP",
        "outputId": "a68e6237-f404-4012-ddaf-a20293978c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# import dataset\n",
        "\n",
        "# importing electron dataset and seperating images and labels\n",
        "#electron_dataset = h5py.File('SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "electron_dataset = h5py.File('/content/drive/MyDrive/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "electron_imgs=np.array(electron_dataset[\"X\"])\n",
        "electron_labels=np.array(electron_dataset[\"y\"],dtype=np.int64)\n",
        "\n",
        "# importing photon dataset and seperating images and labels\n",
        "#photon_dataset = h5py.File('SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "photon_dataset = h5py.File('/content/drive/MyDrive/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "photon_imgs=np.array(photon_dataset[\"X\"])\n",
        "photon_labels=np.array(photon_dataset[\"y\"],dtype=np.int64)\n",
        "\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwVJzUAncjoP",
        "outputId": "835b27be-1058-40d3-bc52-61d6b9b1afb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(249000, 32, 32, 2)\n",
            "float32\n",
            "[1 1 1 ... 1 1 1]\n",
            "(249000, 32, 32, 2)\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(electron_imgs.shape)\n",
        "print(electron_imgs.dtype)\n",
        "print(electron_labels)\n",
        "print(photon_imgs.shape)\n",
        "print(photon_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIzf4BhdcjoQ",
        "outputId": "a067dfb8-8445-4321-8b3d-45f38b29ec76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([498000, 32, 32, 2])\n",
            "torch.Size([498000, 2, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# concatenate electron and photon images/labels\n",
        "img_arrs = torch.Tensor(np.vstack((photon_imgs,electron_imgs)))\n",
        "labels = torch.Tensor(np.hstack((photon_labels,electron_labels))).to(torch.int64)\n",
        "print(img_arrs.shape)\n",
        "img_arrs = img_arrs.permute(0,3,1,2)\n",
        "print(img_arrs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vglbQlP-cjoQ"
      },
      "outputs": [],
      "source": [
        "del electron_imgs,photon_imgs,electron_labels,photon_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UI-5ukT-cjoR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data  # data[:,:,4:-4,4:-4] #to cut external 4 zero padding\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "undmEOTFcjoR",
        "outputId": "4279e6cc-ed84-474c-f2a8-50262e3bddff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "498000\n"
          ]
        }
      ],
      "source": [
        "# Assuming img_arrs and labels are already defined\n",
        "custom_dataset = CustomDataset(img_arrs, labels)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(custom_dataset))\n",
        "valid_size = int(0.2 * len(custom_dataset))\n",
        "test_size = len(custom_dataset) - train_size - valid_size\n",
        "train_dataset, valid_dataset, test_dataset = random_split(custom_dataset, [train_size, valid_size, test_size])\n",
        "print(len(valid_dataset)+len(test_dataset)+len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kskn6moecjoS"
      },
      "source": [
        "transforms should be used here before data loader generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XMdKCz9DcjoT"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                          num_workers=NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og5pOUOjcjoU"
      },
      "source": [
        "\n",
        "\n",
        "Step 1 Tasks:\n",
        "\n",
        "1- turn an image into patches\n",
        "\n",
        "2- flatten the patch feature maps into a single dimension\n",
        "\n",
        "3- Convert the output into Desried output (flattened 2D patches): (196, 768) -> N×(P2⋅C) #Current shape: (1, 768, 196)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7YM6uSfcjoV",
        "outputId": "77071ab7-c04d-4783-af1c-af0424f809b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inputs torch.Size([256, 2, 32, 32]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs = next(iter(train_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs['image'].shape, \"\\n\")\n",
        "# print(\"Full\",data_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUHj_QqDcx8O"
      },
      "source": [
        "# For Max pool:\n",
        "output size = 1+ (input_size-kernel_size)/stride"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1-b_KGAcjoW"
      },
      "outputs": [],
      "source": [
        "# pool=nn.MaxPool2d(kernel_size=4,stride=2)\n",
        "# plt.imshow(data_inputs['image'][0,0:1,:,:].permute(1,2,0))\n",
        "# plt.title('Electron Image ch1')\n",
        "# plt.show()\n",
        "# plt.imshow(pool(data_inputs['image'][0,0:1,:,:]).permute(1,2,0))\n",
        "# plt.show()\n",
        "# plt.imshow(pool(pool(data_inputs['image'][0,0:1,:,:])).permute(1,2,0))\n",
        "# plt.show()\n",
        "# print(pool(pool(data_inputs['image'][0,0:1,:,:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neakyVMUcjoW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "my_conv1 = nn.Conv2d(in_channels=2, out_channels=31, kernel_size=3, padding=1)\n",
        "output=my_conv1((data_inputs['image']))\n",
        "print(output.shape)\n",
        "relu = nn.ReLU()\n",
        "output=output.view(BATCH_SIZE,-1)\n",
        "print(output.shape)\n",
        "fc = nn.Linear(output.shape[1],2)\n",
        "fc(output)\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L0hnqc94cjoX"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1,dropout_rate=DROP_OUT):\n",
        "        super().__init__()\n",
        "        self.my_conv1 = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.gelu = nn.GELU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        #self.my_conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "        #self.my_conv3 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.Linear1=nn.Linear(16*16*8,16*16*2)\n",
        "        self.Linear2=nn.Linear(16*16*2,24*8)\n",
        "        self.Linear3=nn.Linear(24*8,4)\n",
        "        #self.Linear4=nn.Linear(4,2)\n",
        "        # self.Linear1=nn.Linear(16*16*8,8*8*1)\n",
        "        # self.Linear2=nn.Linear(8*8*1,24*8)\n",
        "        # self.Linear3=nn.Linear(24*8,4)\n",
        "        self.Linear=nn.Linear(4,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x['image'] =self.my_conv1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.pool(x['image'])\n",
        "        x['image'] =x['image'].view(BATCH_SIZE,-1)\n",
        "\n",
        "        x['image'] =self.Linear1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear2(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear3(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.Linear(x['image'])\n",
        "        return x['image']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MKr6DGwEcjoY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class My2Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=4,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv2=nn.Conv2d(in_channels=4,out_channels=8,kernel_size=3,stride=1,padding=3)\n",
        "        #self.conv3=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv4=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv5=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
        "        #self.bn1=nn.BatchNorm2d(16)\n",
        "        #self.bn2=nn.BatchNorm2d(64)\n",
        "        #self.bn2=nn.BatchNorm2d(32)\n",
        "        #self.b3=nn.BatchNorm2d(16)\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.unfold = nn.Unfold(kernel_size=3, stride=1, padding=1)\n",
        "        self.sparse=nn.Conv2d(in_channels=3*3,out_channels=2,kernel_size=1)\n",
        "\n",
        "        self.relu=nn.ReLU()\n",
        "        self.dropout=nn.Dropout(0.1)\n",
        "        #self.fc0=nn.Linear(12*4*2,12*4) #\n",
        "        self.fc1=nn.Linear(12*4*2,32)\n",
        "        self.fc2=nn.Linear(32,16)\n",
        "        self.fc3=nn.Linear(16,8)\n",
        "        self.fc4=nn.Linear(8,4)\n",
        "        self.fc5=nn.Linear(4,2)\n",
        "        self.out=nn.Linear(2,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel1 = x['image'][:, 0:1, :, :]  # Extract first channel\n",
        "        channel2 = x['image'][:, 1:2, :, :]  # Extract second channel\n",
        "        #print(channel1.shape)\n",
        "        convchannel1 = self.conv1(channel1)\n",
        "        convchannel2 = self.conv1(channel2)\n",
        "        #print(convchannel1.shape)\n",
        "\n",
        "        '''\n",
        "        convchannel1 = convchannel1.view(convchannel1.size(0), -1)\n",
        "        convchannel2 = convchannel2.view(convchannel2.size(0), -1)\n",
        "        convchannel = torch.cat([convchannel1, convchannel2], dim=1)\n",
        "        '''\n",
        "\n",
        "        # Compute normalized histograms for each output channel of the convolutional layer\n",
        "        hist_conv = []\n",
        "        for batch in  convchannel1:# data_inputs['image']:\n",
        "            batch_hist = []\n",
        "            for channel in batch:\n",
        "                # Create a mask of non-zero elements\n",
        "                mask = channel != 0\n",
        "                # Filter the tensor using the mask\n",
        "                #non_zero_elements = channel[mask]\n",
        "                non_zero_elements = channel[mask]*channel[mask]\n",
        "                #min_val=non_zero_elements.min()\n",
        "                max_val=non_zero_elements.max()\n",
        "                # Perform min-max normalization to scale values between 0 and 1\n",
        "                #normalized_tensor = 2 * (non_zero_elements - min_val) / (max_val - min_val) - 1\n",
        "                normalized_tensor = non_zero_elements/max_val\n",
        "                # Compute histogram\n",
        "                # Set all elements within the tensor to zero\n",
        "                hist = torch.histc(normalized_tensor, bins=12, min=0, max=+1)\n",
        "                # Normalize the histogram and append to the list\n",
        "                batch_hist.append(hist)\n",
        "\n",
        "            # Concatenate the histograms for all channels in the batch\n",
        "            batch_hist = torch.cat(batch_hist, dim=0)\n",
        "            hist_conv.append(batch_hist)\n",
        "\n",
        "        # Stack the histograms along a new dimension to get the desired tensor shape\n",
        "        histograms_tensor1 = torch.stack(hist_conv, dim=0).to(device='cuda')\n",
        "        # Shape will be (batch_size, channel_no * bins)\n",
        "        #print(\"MyModel\")\n",
        "        #print(histograms_tensor1.shape)\n",
        "        # hist_ch1 = self.fc0(histograms_tensor)\n",
        "        # print('hello')\n",
        "        # print(hist_ch1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Compute normalized histograms for each output channel of the convolutional layer\n",
        "        hist_conv = []\n",
        "        for batch in  convchannel2:# data_inputs['image']:\n",
        "            batch_hist = []\n",
        "            for channel in batch:\n",
        "                # Create a mask of non-zero elements\n",
        "                mask = channel != 0\n",
        "                # Filter the tensor using the mask\n",
        "                #non_zero_elements = channel[mask]\n",
        "                non_zero_elements = channel[mask]*channel[mask]\n",
        "                #min_val=non_zero_elements.min()\n",
        "                max_val=non_zero_elements.max()\n",
        "                # Perform min-max normalization to scale values between 0 and 1\n",
        "                #normalized_tensor = 2 * (non_zero_elements - min_val) / (max_val - min_val) - 1\n",
        "                normalized_tensor = non_zero_elements/max_val\n",
        "                # Compute histogram\n",
        "                # Set all elements within the tensor to zero\n",
        "                hist = torch.histc(normalized_tensor, bins=12, min=0, max=+1)\n",
        "                # Normalize the histogram and append to the list\n",
        "                batch_hist.append(hist)\n",
        "\n",
        "            # Concatenate the histograms for all channels in the batch\n",
        "            batch_hist = torch.cat(batch_hist, dim=0)\n",
        "            hist_conv.append(batch_hist)\n",
        "\n",
        "        # Stack the histograms along a new dimension to get the desired tensor shape\n",
        "        histograms_tensor2 = torch.stack(hist_conv, dim=0).to(device='cuda')\n",
        "        # Shape will be (batch_size, channel_no * bins)\n",
        "        #print(\"MyModel\")\n",
        "        #print(histograms_tensor2.shape)\n",
        "        # hist_ch2 =self.fc0(histograms_tensor)\n",
        "        # print('hello')\n",
        "        # print(hist_ch2.shape)\n",
        "\n",
        "\n",
        "        convchannel = torch.cat([histograms_tensor1, histograms_tensor2], dim=1)\n",
        "        #print(convchannel.shape)\n",
        "\n",
        "        convchannel = self.fc1(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc2(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc3(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc4(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc5(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "\n",
        "        convchannel = self.out(convchannel)\n",
        "\n",
        "        return convchannel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlGKB3b_cjoZ",
        "outputId": "53eafbeb-30bb-4c7c-a379-d50986061851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My2Model(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (unfold): Unfold(kernel_size=3, dilation=1, padding=1, stride=1)\n",
            "  (sparse): Conv2d(9, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=96, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (fc4): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (fc5): Linear(in_features=4, out_features=2, bias=True)\n",
            "  (out): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = My2Model()\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHzGdHmCcx8P",
        "outputId": "1884ca01-57d0-481b-8daa-ae0112320622"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# clearing cuda cache memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "U6CZL37tcjob"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "N8Gfnbybcjoc"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "loss_module = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, data_loader, val_loader,loss_module, num_epochs=EPOCHS):\n",
        "    train_losses = []  # Changed variable name from train_loss to train_losses\n",
        "    val_losses = []\n",
        "\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs in data_loader:\n",
        "            # Check if the current batch is the last one\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            #print(preds.shape)\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * BATCH_SIZE # batch_size\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        #validation plase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "          for data_inputs in val_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "            running_loss += loss.item() * BATCH_SIZE\n",
        "        val_loss = running_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss {train_loss}, Validation Loss {val_loss}\")\n",
        "        eval_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOKDNgKEcjoe",
        "outputId": "13ac813c-8d3f-4040-e9ab-ee9b69d8376b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/40 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40 - Train loss 0.6759177661749117, Validation Loss 0.6706020102443465\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▎         | 1/40 [14:58<9:44:06, 898.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 59.48%\n",
            "Epoch 2/40 - Train loss 0.8585479814679337, Validation Loss 0.6664018801034215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 2/40 [29:46<9:25:08, 892.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 59.69%\n",
            "Epoch 3/40 - Train loss 0.8552128612017126, Validation Loss 0.668077109769645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 3/40 [44:29<9:07:42, 888.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 59.87%\n",
            "Epoch 4/40 - Train loss 0.8545491718110584, Validation Loss 0.6647989953282368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 4/40 [59:10<8:51:03, 885.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 59.96%\n"
          ]
        }
      ],
      "source": [
        "train_model(model, optimizer, train_loader, val_loader, loss_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrkhA9fmcjoe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    return acc\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a88zbsm1cjoe",
        "outputId": "b2f7645d-a3ce-4692-8aa0-ec9b52faf150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7020, device='cuda:0')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#eval_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}