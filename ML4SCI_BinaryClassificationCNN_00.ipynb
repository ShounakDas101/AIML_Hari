{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShounakDas101/AIML_Hari/blob/main/ML4SCI_BinaryClassificationCNN_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrgT5oYocjoG",
        "outputId": "77c11a21-f03c-4149-d2c6-5e03a1901d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import h5py\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gfDwFQPedS0Y",
        "outputId": "58c89224-194f-4a78-f4ff-6ecc5aa29fa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f5axppRQcjoN"
      },
      "outputs": [],
      "source": [
        "DATA_DIR='./data'\n",
        "'''\n",
        "IMAGE_SIZE = 32 # W=32 H=32\n",
        "NUM_CHANNELS = 2 # R G B then it should be 3\n",
        "PATCH_SIZE = 4 # W_patch=4 H_patch=4\n",
        "NUM_PATCHES = (IMAGE_SIZE//PATCH_SIZE)*(IMAGE_SIZE//PATCH_SIZE) # = 64\n",
        "NUM_HEADS = 8  # enbedding dim should be divisible by no of heads\n",
        "EMBBEDING_DIM = PATCH_SIZE*PATCH_SIZE*NUM_CHANNELS  #NUM_PATCHNELS\n",
        "MLP_SIZE = 128\n",
        "NUM_LAYERS = 8\n",
        "'''\n",
        "NUM_CLASSES = 2\n",
        "NUM_WORKERS = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h6-3QpHUcjoO",
        "outputId": "6caaa767-21ce-410b-c459-e8c54dc14b0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# clearing cuda cache memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAO9GsjVcjoO",
        "outputId": "c597c363-a462-4bc2-c6c8-6eee643857f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file '/content/drive/MyDrive/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5' exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the file in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "# CHECKPOINT_PATH = '/content/drive/MyDrive/CHECKPOINTS'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"The file '{file_path}' exists.\")\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quSZE_AicjoP"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "'''\n",
        "def find_keys(hdf5_obj, path='/'):\n",
        "    \"\"\"Recursively find keys in an HDF5 file.\"\"\"\n",
        "    keys = []\n",
        "    for key in hdf5_obj[path].keys():\n",
        "        full_path = f\"{path}/{key}\"\n",
        "        keys.append(full_path)\n",
        "        if isinstance(hdf5_obj[full_path], h5py.Group):\n",
        "            keys.extend(find_keys(hdf5_obj, full_path))\n",
        "    return keys\n",
        "\n",
        "# Open the HDF5 file\n",
        "file_path = 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'  # Replace with your actual file path\n",
        "with h5py.File(file_path, 'r') as hdf5_file:\n",
        "    # Find keys starting from the root\n",
        "    all_keys = find_keys(hdf5_file)\n",
        "\n",
        "# Print the found keys\n",
        "for key in all_keys:\n",
        "    print(key)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJfT42R9cjoP",
        "outputId": "d8c6f6fd-bb5f-4857-90e4-dc7915721589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# import dataset\n",
        "\n",
        "# importing electron dataset and seperating images and labels\n",
        "electron_dataset = h5py.File('/content/drive/MyDrive/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "electron_imgs=np.array(electron_dataset[\"X\"])\n",
        "electron_labels=np.array(electron_dataset[\"y\"],dtype=np.int64)\n",
        "\n",
        "# importing photon dataset and seperating images and labels\n",
        "photon_dataset = h5py.File('/content/drive/MyDrive/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5',\"r\")\n",
        "photon_imgs=np.array(photon_dataset[\"X\"])\n",
        "photon_labels=np.array(photon_dataset[\"y\"],dtype=np.int64)\n",
        "\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WwVJzUAncjoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf3fe31-08c9-43ac-af41-f5f52a979510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(249000, 32, 32, 2)\n",
            "float32\n",
            "[1 1 1 ... 1 1 1]\n",
            "(249000, 32, 32, 2)\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(electron_imgs.shape)\n",
        "print(electron_imgs.dtype)\n",
        "print(electron_labels)\n",
        "print(photon_imgs.shape)\n",
        "print(photon_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIzf4BhdcjoQ",
        "outputId": "11142c64-00cf-4709-ae11-4f995973b523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([498000, 32, 32, 2])\n",
            "torch.Size([498000, 2, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# concatenate electron and photon images/labels\n",
        "img_arrs = torch.Tensor(np.vstack((photon_imgs,electron_imgs)))\n",
        "labels = torch.Tensor(np.hstack((photon_labels,electron_labels))).to(torch.int64)\n",
        "print(img_arrs.shape)\n",
        "img_arrs = img_arrs.permute(0,3,1,2)\n",
        "print(img_arrs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vglbQlP-cjoQ"
      },
      "outputs": [],
      "source": [
        "del electron_imgs,photon_imgs,electron_labels,photon_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UI-5ukT-cjoR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data  # data[:,:,4:-4,4:-4] #to cut external 4 zero padding\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "undmEOTFcjoR",
        "outputId": "4ab7ed16-4622-4438-959a-d79a256de9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "498000\n"
          ]
        }
      ],
      "source": [
        "# Assuming img_arrs and labels are already defined\n",
        "custom_dataset = CustomDataset(img_arrs, labels)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(custom_dataset))\n",
        "valid_size = int(0.2 * len(custom_dataset))\n",
        "test_size = len(custom_dataset) - train_size - valid_size\n",
        "train_dataset, valid_dataset, test_dataset = random_split(custom_dataset, [train_size, valid_size, test_size])\n",
        "print(len(valid_dataset)+len(test_dataset)+len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kskn6moecjoS"
      },
      "source": [
        "transforms should be used here before data loader generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XMdKCz9DcjoT"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                          num_workers=NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og5pOUOjcjoU"
      },
      "source": [
        "\n",
        "\n",
        "Step 1 Tasks:\n",
        "\n",
        "1- turn an image into patches\n",
        "\n",
        "2- flatten the patch feature maps into a single dimension\n",
        "\n",
        "3- Convert the output into Desried output (flattened 2D patches): (196, 768) -> N×(P2⋅C) #Current shape: (1, 768, 196)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e7YM6uSfcjoV",
        "outputId": "d755cc24-0791-4d65-d8ba-9e179ed9f6dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data inputs torch.Size([512, 2, 32, 32]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs = next(iter(train_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs['image'].shape, \"\\n\")\n",
        "# print(\"Full\",data_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "v1-b_KGAcjoW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "0dda33eb-7ab3-422e-ee64-80761c24b7ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(data_inputs['image'][0,0:1,:,:].permute(1,2,0))  # Assuming it's a 2-channel image; adjust if needed  cmap='gray'\n",
        "plt.title('Electron Image ch1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neakyVMUcjoW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "my_conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "output=my_conv1((data_inputs['image'][:,0:1,:,:]))\n",
        "print(output.shape)\n",
        "relu = nn.ReLU()\n",
        "output=output.view(BATCH_SIZE,-1)\n",
        "print(output.shape)\n",
        "fc = nn.Linear(output.shape[1],2)\n",
        "fc(output)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0hnqc94cjoX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1,dropout_rate=.2):\n",
        "        super().__init__()\n",
        "        self.my_conv1 = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.my_conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.my_conv3 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.Linear1=nn.Linear(24*24*8,24*24*1)\n",
        "        self.Linear2=nn.Linear(24*24*1,24*8)\n",
        "        self.Linear3=nn.Linear(24*8,4)\n",
        "\n",
        "        self.Linear=nn.Linear(4,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x['image'] =self.my_conv1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.my_conv2(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.my_conv3(x['image'])\n",
        "        x['image'] =x['image'].view(BATCH_SIZE,-1)\n",
        "\n",
        "        x['image'] =self.Linear1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear2(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear3(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear(x['image'])\n",
        "        return x['image']\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MKr6DGwEcjoY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class My2Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1=nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
        "        self.conv2=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv3=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv4=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1)\n",
        "        #self.conv5=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1)\n",
        "        self.b1=nn.BatchNorm2d(16)\n",
        "        self.b2=nn.BatchNorm2d(32)\n",
        "        #self.b3=nn.BatchNorm2d(16)\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.relu=nn.ReLU()\n",
        "        self.dropout=nn.Dropout(0.1)\n",
        "        self.fc1=nn.Linear(8*8*32+8*8*32,8*8*32)\n",
        "        self.fc2=nn.Linear(8*8*32,8*8)\n",
        "        self.fc3=nn.Linear(8*8,16)\n",
        "        self.fc4=nn.Linear(16,8)\n",
        "        self.fc5=nn.Linear(8,2)\n",
        "        self.out=nn.Linear(32,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        channel1 = x['image'][:, 0:1, :, :]  # Extract first channel\n",
        "        channel2 = x['image'][:, 1:2, :, :]  # Extract second channel\n",
        "\n",
        "        convchannel1=self.relu(self.b1(self.conv1(self.pool(self.pool(channel1)))))\n",
        "        convchannel2=self.relu(self.b1(self.conv1(self.pool(self.pool(channel2)))))\n",
        "\n",
        "        convchannel1 = convchannel1.view(convchannel1.size(0), -1)\n",
        "        convchannel2 = convchannel2.view(convchannel2.size(0), -1)\n",
        "        convchannel = torch.cat([convchannel1, convchannel2], dim=1)\n",
        "\n",
        "        #convchannel = self.out(convchannel)\n",
        "\n",
        "\n",
        "        '''\n",
        "        convchannel = self.fc1(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc2(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc3(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc4(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc5(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.dropout(convchannel)\n",
        "        '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        '''\n",
        "        #working code\n",
        "\n",
        "        convchannel1=self.pool(self.relu(self.b1(self.conv1(channel1))))\n",
        "        convchannel1=self.pool(self.relu(self.b2(self.conv2(convchannel1))))\n",
        "        convchannel1 = convchannel1.view(convchannel1.size(0), -1)\n",
        "\n",
        "        convchannel2=self.pool(self.relu(self.b1(self.conv1(channel2))))\n",
        "        convchannel2=self.pool(self.relu(self.b2(self.conv2(convchannel2))))\n",
        "        #convchannel2=self.pool(self.relu(self.b3(self.conv3(convchannel2))))\n",
        "        convchannel2 = convchannel2.view(convchannel2.size(0), -1)\n",
        "\n",
        "        convchannel = torch.cat([convchannel1, convchannel2], dim=1)\n",
        "\n",
        "        convchannel = self.fc1(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc2(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc3(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc4(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.fc5(convchannel)\n",
        "        convchannel = self.relu(convchannel)\n",
        "\n",
        "        convchannel = self.dropout(convchannel)\n",
        "\n",
        "        convchannel = self.out(convchannel)\n",
        "        '''\n",
        "\n",
        "        '''\n",
        "        x=self.pool(F.relu(self.b1(self.conv1(x))))\n",
        "        x=self.pool(F.relu(self.conv2(x)))\n",
        "        x=self.pool(F.relu(self.b2(self.conv3(x))))\n",
        "        x=self.pool(F.relu(self.conv4(x)))\n",
        "        x=self.pool(F.relu(self.b3(self.conv5(x))))\n",
        "        x=x.view(-1,256)\n",
        "        x = self.dropout(x)\n",
        "        x=self.dropout(F.relu(self.fc1(x)))\n",
        "        x=self.dropout(F.relu(self.fc2(x)))\n",
        "        x=self.out(x)\n",
        "        '''\n",
        "\n",
        "        return convchannel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XlGKB3b_cjoZ",
        "outputId": "772b3423-c0f1-4098-a042-1a4b6ae878f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My2Model(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (b1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (b2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "  (fc2): Linear(in_features=2048, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=16, bias=True)\n",
            "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (fc5): Linear(in_features=8, out_features=2, bias=True)\n",
            "  (out): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = My2Model(num_classes=1)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "U6CZL37tcjob"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "N8Gfnbybcjoc"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "loss_module = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, data_loader, val_loader,loss_module, num_epochs=EPOCHS):\n",
        "    train_losses = []  # Changed variable name from train_loss to train_losses\n",
        "    val_losses = []\n",
        "\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs in data_loader:\n",
        "            # Check if the current batch is the last one\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * BATCH_SIZE # batch_size\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        #validation plase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "          for data_inputs in val_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "            running_loss += loss.item() * BATCH_SIZE\n",
        "        val_loss = running_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss {train_loss}, Validation Loss {val_loss}\")\n",
        "        eval_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "NOKDNgKEcjoe",
        "outputId": "b21ef854-3d51-48db-97c2-762b0f9a017b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-df4d758e25d7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-aa6c693e572d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, data_loader, val_loader, loss_module, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m## Step 3: Calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m## Step 4: Perform backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    726\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([512])) must be the same as input size (torch.Size([512, 2048]))"
          ]
        }
      ],
      "source": [
        "train_model(model, optimizer, train_loader, val_loader, loss_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrkhA9fmcjoe"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    return acc\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a88zbsm1cjoe",
        "outputId": "b2f7645d-a3ce-4692-8aa0-ec9b52faf150"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7020, device='cuda:0')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#eval_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}