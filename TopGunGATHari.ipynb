{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShounakDas101/AIML_Hari/blob/main/TopGunGATHari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k2bhD2R7moni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c1ee4f-d8ba-40fd-a965-94882c8ab850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WMKUxKbmlqa"
      },
      "outputs": [],
      "source": [
        "# import pyarrow.parquet as pq\n",
        "\n",
        "# topgun_file ='/content/drive/MyDrive/data/raw/output_2048_lines.parquet'\n",
        "# # Read the Parquet file into a PyArrow Table\n",
        "# parquet_table = pq.read_table(topgun_file)\n",
        "# # Convert the PyArrow Table to a pandas DataFrame\n",
        "# data_frame = parquet_table.to_pandas()\n",
        "# # Get the keys (column names) of the DataFrame\n",
        "# keys = data_frame.columns.tolist()  # Convert the column names to a list\n",
        "# print(keys)\n",
        "# # data_frame['m']\n",
        "# print(data_frame['m'],data_frame['iphi'],data_frame['pt'],data_frame['ieta'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "QqADiiuAIjjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457dccab-0b79-4a18-c007-99bffe34712b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prcy8ud8mlqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19aba437-c2f5-4331-f48b-e1a7b8b58f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9Gr4XDlmlqd"
      },
      "outputs": [],
      "source": [
        "import torch_geometric\n",
        "import numpy as np\n",
        "from torch_geometric.utils import get_laplacian, to_scipy_sparse_matrix, to_dense_adj, to_undirected\n",
        "\n",
        "\n",
        "def eigvec_normalizer(EigVecs, EigVals, normalization=\"L2\", eps=1e-12):\n",
        "    \"\"\"\n",
        "    Implement different eigenvector normalizations.\n",
        "    \"\"\"\n",
        "\n",
        "    EigVals = EigVals.unsqueeze(0)\n",
        "\n",
        "    if normalization == \"L1\":\n",
        "        # L1 normalization: eigvec / sum(abs(eigvec))\n",
        "        denom = EigVecs.norm(p=1, dim=0, keepdim=True)\n",
        "\n",
        "    elif normalization == \"L2\":\n",
        "        # L2 normalization: eigvec / sqrt(sum(eigvec^2))\n",
        "        denom = EigVecs.norm(p=2, dim=0, keepdim=True)\n",
        "\n",
        "    elif normalization == \"abs-max\":\n",
        "        # AbsMax normalization: eigvec / max|eigvec|\n",
        "        denom = torch.max(EigVecs.abs(), dim=0, keepdim=True).values\n",
        "\n",
        "    elif normalization == \"wavelength\":\n",
        "        # AbsMax normalization, followed by wavelength multiplication:\n",
        "        # eigvec * pi / (2 * max|eigvec| * sqrt(eigval))\n",
        "        denom = torch.max(EigVecs.abs(), dim=0, keepdim=True).values\n",
        "        eigval_denom = torch.sqrt(EigVals)\n",
        "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
        "        denom = denom * eigval_denom * 2 / np.pi\n",
        "\n",
        "    elif normalization == \"wavelength-asin\":\n",
        "        # AbsMax normalization, followed by arcsin and wavelength multiplication:\n",
        "        # arcsin(eigvec / max|eigvec|)  /  sqrt(eigval)\n",
        "        denom_temp = torch.max(EigVecs.abs(), dim=0, keepdim=True).values.clamp_min(eps).expand_as(EigVecs)\n",
        "        EigVecs = torch.asin(EigVecs / denom_temp)\n",
        "        eigval_denom = torch.sqrt(EigVals)\n",
        "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
        "        denom = eigval_denom\n",
        "\n",
        "    elif normalization == \"wavelength-soft\":\n",
        "        # AbsSoftmax normalization, followed by wavelength multiplication:\n",
        "        # eigvec / (softmax|eigvec| * sqrt(eigval))\n",
        "        denom = (F.softmax(EigVecs.abs(), dim=0) * EigVecs.abs()).sum(dim=0, keepdim=True)\n",
        "        eigval_denom = torch.sqrt(EigVals)\n",
        "        eigval_denom[EigVals < eps] = 1  # Problem with eigval = 0\n",
        "        denom = denom * eigval_denom\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported normalization `{normalization}`\")\n",
        "\n",
        "    denom = denom.clamp_min(eps).expand_as(EigVecs)\n",
        "    EigVecs = EigVecs / denom\n",
        "\n",
        "    return EigVecs\n",
        "\n",
        "def get_lap_decomp_stats(evals, evects, max_freqs, eigvec_norm='L2'):\n",
        "\n",
        "    N = len(evals)  # Number of nodes, including disconnected nodes.\n",
        "\n",
        "    # Keep up to the maximum desired number of frequencies.\n",
        "    idx = evals.argsort()[:max_freqs]\n",
        "    evals, evects = evals[idx], np.real(evects[:, idx])\n",
        "    evals = torch.from_numpy(np.real(evals)).clamp_min(0)\n",
        "\n",
        "    # Normalize and pad eigen vectors.\n",
        "    evects = torch.from_numpy(evects).float()\n",
        "    evects = eigvec_normalizer(evects, evals, normalization=eigvec_norm)\n",
        "    if N < max_freqs:\n",
        "        EigVecs = F.pad(evects, (0, max_freqs - N), value=float('nan'))\n",
        "    else:\n",
        "        EigVecs = evects\n",
        "\n",
        "    # Pad and save eigenvalues.\n",
        "    if N < max_freqs:\n",
        "        EigVals = F.pad(evals, (0, max_freqs - N), value=float('nan')).unsqueeze(0)\n",
        "    else:\n",
        "        EigVals = evals.unsqueeze(0)\n",
        "    EigVals = EigVals.repeat(N, 1).unsqueeze(2)\n",
        "\n",
        "    return EigVals, EigVecs\n",
        "\n",
        "def compute_enc_transform(x, edge_index, transform_flags):\n",
        "\n",
        "    N = x.shape[0]\n",
        "\n",
        "    to_return = {}\n",
        "\n",
        "    if transform_flags[\"LapPE\"]:\n",
        "        undir_edge_idx = to_undirected(edge_index, num_nodes  = N)\n",
        "        L = to_scipy_sparse_matrix(\n",
        "            *get_laplacian(undir_edge_idx, normalization=transform_flags[\"LapPEnorm\"], num_nodes=N)\n",
        "        )\n",
        "        evals, evects = np.linalg.eigh(L.toarray())\n",
        "        max_freqs = transform_flags[\"LapPEmax_freq\"]\n",
        "        eigvec_norm = transform_flags[\"LapPEeig_norm\"]\n",
        "\n",
        "        EigVals, EigVecs = get_lap_decomp_stats(\n",
        "            evals=evals, evects=evects,\n",
        "            max_freqs=max_freqs,\n",
        "            eigvec_norm=eigvec_norm\n",
        "        )\n",
        "\n",
        "        to_return['eigvals'] = EigVals\n",
        "        to_return['eigvecs'] = EigVecs\n",
        "    return to_return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKy69HDMmlqe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f367a955-fd7e-431a-a3b0-31903e59bd18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef positional_encoding(data, pe_scales):\\n    pe_cos = torch.cat([torch.cos(2**i * np.pi * torch.as_tensor(data))\\n                       for i in range(pe_scales)], dim=1)\\n    pe_sin = torch.cat([torch.sin(2**i * np.pi * torch.as_tensor(data))\\n                       for i in range(pe_scales)], dim=1)\\n\\n    output= torch.cat([torch.as_tensor(data), pe_cos, pe_sin], dim=1)\\n    return output\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''\n",
        "def positional_encoding(data, pe_scales):\n",
        "    pe_cos = torch.cat([torch.cos(2**i * np.pi * torch.as_tensor(data))\n",
        "                       for i in range(pe_scales)], dim=1)\n",
        "    pe_sin = torch.cat([torch.sin(2**i * np.pi * torch.as_tensor(data))\n",
        "                       for i in range(pe_scales)], dim=1)\n",
        "\n",
        "    output= torch.cat([torch.as_tensor(data), pe_cos, pe_sin], dim=1)\n",
        "    return output\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cMti47Vmlqf"
      },
      "outputs": [],
      "source": [
        "def normalize_x(x):\n",
        "    x = x - np.array([0.01037084, 0.0103173, 0.01052679, 0.01034378, 0.01097225, 0.01024814, 0.01037642, 0.01058754])\n",
        "    x = x / np.array([10.278656283775618, 7.64753320751208, 16.912319597559645, 9.005579923580713, 21.367327333103688, 7.489890622699373, 12.977402491253788, 24.50774893130742])\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_D1_TLemlqf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "hcal_scale  = 1\n",
        "ecal_scale  = 0.1\n",
        "pt_scale    = 0.01\n",
        "dz_scale    = 0.05\n",
        "d0_scale    = 0.1\n",
        "m0_scale    = 85\n",
        "m1_scale    = 415\n",
        "p0_scale = 400\n",
        "p1_scale = 600\n",
        "\n",
        "def points_all_channels(X_jets, suppression_thresh):\n",
        "    idx = np.where(abs(X_jets).sum(axis=0) > suppression_thresh)\n",
        "    pos = np.array(idx).T / X_jets.shape[1]\n",
        "    x = X_jets[:, idx[0], idx[1]].T\n",
        "\n",
        "    return x, pos\n",
        "\n",
        "class PointCloudFromParquetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,data_dir,save_data,id,filename,\n",
        "                    transform_flags,suppresion_thresh,k,min_mass,max_mass,num_bins):\n",
        "        super().__init__()\n",
        "\n",
        "        self.id = id\n",
        "        self.file = pq.ParquetFile(filename)\n",
        "        self.root_dir = data_dir\n",
        "        self.save_data = save_data\n",
        "        self.transform_flags = transform_flags\n",
        "        self.suppression_thresh = suppresion_thresh\n",
        "        self.k = k\n",
        "        self.min_mass=min_mass\n",
        "        self.max_mass=max_mass\n",
        "        self.num_bins=num_bins\n",
        "\n",
        "        bin_size=(max_mass-min_mass)/num_bins\n",
        "        self.bins=[min_mass + i*bin_size for i in range(num_bins)]\n",
        "\n",
        "    def __getitem__(self, idx, ):\n",
        "        row = self.file.read_row_group(idx).to_pydict()\n",
        "        arr = np.array(row['X_jet'][0])\n",
        "        x, pos = points_all_channels(arr, self.suppression_thresh)\n",
        "        x = normalize_x(x)\n",
        "        pt = row['pt'][0]\n",
        "        ieta = row['ieta'][0]\n",
        "        iphi = row['iphi'][0]\n",
        "        m = row['m'][0]\n",
        "        m=m-m0_scale\n",
        "        m=m/m1_scale\n",
        "        m_class= (-1)\n",
        "        x[:, 0] *= pt_scale\n",
        "        x[:, 1] *= dz_scale\n",
        "        x[:, 2] *= d0_scale\n",
        "        x[:, 3] *= ecal_scale\n",
        "        x[:, 4] *= hcal_scale\n",
        "        pt = (pt - p0_scale) / p1_scale\n",
        "        iphi = iphi / 360.\n",
        "        ieta = ieta / 140.\n",
        "\n",
        "                        # High value suppression\n",
        "        x[:, 1][x[:, 1] < -1] = 0\n",
        "        x[:, 1][x[:, 1] > 1] = 0\n",
        "        x[:, 2][x[:, 2] < -1] = 0\n",
        "        x[:, 2][x[:, 2] > 1] = 0\n",
        "\n",
        "                        # Zero suppression\n",
        "        x[:, 0][x[:, 0] < 1e-2] = 0.\n",
        "        x[:, 3][x[:, 3] < 1e-2] = 0.\n",
        "        x[:, 4][x[:, 4] < 1e-2] = 0.\n",
        "        for it,bin_start in enumerate(self.bins):\n",
        "            if bin_start > m:\n",
        "                m_class= it -1\n",
        "                break\n",
        "        if m_class == -1:\n",
        "            m_class=self.num_bins -1\n",
        "        y_class=m_class\n",
        "        x = np.concatenate([x, pos], axis=1)\n",
        "        pos = torch.as_tensor(pos, dtype=torch.float)\n",
        "        x = torch.as_tensor(x)\n",
        "        edge_index = torch_geometric.nn.knn_graph(x=pos, k = self.k, num_workers=0)\n",
        "        transforms = compute_enc_transform(x, edge_index, self.transform_flags)\n",
        "        print(transforms['eigvecs'].shape)\n",
        "        print(transforms['eigvals'].shape)\n",
        "        x = torch.cat([x, transforms['eigvecs'], transforms['eigvals'].squeeze(-1)], dim=-1)\n",
        "        print(x.shape)\n",
        "        data = torch_geometric.data.Data(\n",
        "            pos=pos.float(),x=x.float(),\n",
        "            pt=torch.as_tensor(pt).unsqueeze(-1),\n",
        "            ieta=torch.as_tensor(ieta).unsqueeze(-1),\n",
        "            iphi=torch.as_tensor(iphi).unsqueeze(-1),\n",
        "            y=torch.as_tensor(m),\n",
        "            y_class=y_class\n",
        "        )\n",
        "        if self.save_data:\n",
        "            torch.save(data, os.path.join(self.save_data, f'{self.id}_{idx}.pt'))\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.file.num_row_groups\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=PointCloudFromParquetDataset(\n",
        "                    data_dir = '/content/drive/MyDrive/data/',\n",
        "                    save_data = False,\n",
        "                    id = 0,\n",
        "                    filename = '/content/drive/MyDrive/data/raw/top_gun_opendata_1.parquet',\n",
        "                    transform_flags = {\"LapPE\": True,\"LapPEnorm\": \"sym\",\"LapPEmax_freq\": 10,\"LapPEeig_norm\": \"L2\",\"RWSE\": False,\"RWSEkernel_times\": [2, 3, 5, 7, 10]},\n",
        "                    suppresion_thresh=1e-3,\n",
        "                    k = 20,min_mass=0,max_mass=1,num_bins=10\n",
        "                )"
      ],
      "metadata": {
        "id": "X_qEi5McaZQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "id": "I9Pn0sz1b9Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf07901-0cda-45ed-d96f-5addb655c403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150165"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3yVoEZbmlqg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class TopGunPreprocessor():\n",
        "    def __init__(self,data_dir,num_files,test_ratio,val_ratio,transform_flags,min_threshold,k,min_mass=0,max_mass=40,num_bins=4):\n",
        "        self.data_dir=data_dir\n",
        "        self.num_files=num_files\n",
        "        self.test_ratio=test_ratio\n",
        "        self.val_ratio=val_ratio\n",
        "        self.transform_flags=transform_flags\n",
        "        self.min_threshold=min_threshold\n",
        "        self.k=k\n",
        "        self.min_mass=min_mass\n",
        "        self.max_mass=max_mass\n",
        "        self.num_bins=num_bins\n",
        "        self.preprocess()\n",
        "\n",
        "    def preprocess(self):\n",
        "        paths = list(glob.glob(os.path.join(self.data_dir, \"raw\", \"*.parquet\")))\n",
        "\n",
        "        dsets = []\n",
        "        for it,path in enumerate(tqdm(paths[0:self.num_files])):\n",
        "            dsets.append(\n",
        "                PointCloudFromParquetDataset(\n",
        "                    data_dir = self.data_dir,\n",
        "                    save_data = os.path.join(self.data_dir, \"saved\"),\n",
        "                    id = it,\n",
        "                    filename = path,\n",
        "                    transform_flags = self.transform_flags,\n",
        "                    suppresion_thresh=self.min_threshold,\n",
        "                    k = self.k,min_mass=self.min_mass,max_mass=self.max_mass,num_bins=self.num_bins\n",
        "                )\n",
        "            )\n",
        "\n",
        "        combined_dset = torch.utils.data.ConcatDataset(dsets)\n",
        "\n",
        "        sampled_data_size = int(len(combined_dset) * 0.005)\n",
        "\n",
        "        random_indices = random.sample(range(len(combined_dset)), sampled_data_size)\n",
        "\n",
        "        combined_dset = torch.utils.data.Subset(combined_dset, random_indices)\n",
        "\n",
        "        val_size = int(len(combined_dset) * self.val_ratio)\n",
        "        test_size = int(len(combined_dset) * self.test_ratio)\n",
        "        train_size = len(combined_dset) - val_size - test_size\n",
        "\n",
        "        train_dset, val_dset, test_dset = torch.utils.data.random_split(\n",
        "            combined_dset,\n",
        "            [train_size, val_size, test_size],\n",
        "            generator=torch.Generator().manual_seed(42),\n",
        "        )\n",
        "\n",
        "        self.train_dataset = train_dset\n",
        "        self.val_dataset = val_dset\n",
        "        self.test_dataset = test_dset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6syAmJEhmlqh"
      },
      "outputs": [],
      "source": [
        "dset = TopGunPreprocessor(data_dir='/content/drive/MyDrive/data/',num_files=1,test_ratio=.2,val_ratio=.2,transform_flags={\"LapPE\": True,\"LapPEnorm\": \"sym\",\"LapPEmax_freq\": 10,\"LapPEeig_norm\": \"L2\",\"RWSE\": False,\"RWSEkernel_times\": [2, 3, 5, 7, 10]},min_threshold=1e-3,k = 20,min_mass=0,max_mass=1,num_bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BsJ8Xc6mlqh"
      },
      "outputs": [],
      "source": [
        "train_dataset=dset.train_dataset\n",
        "val_dataset = dset.val_dataset\n",
        "test_dataset = dset.test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dibWecsKmlqi"
      },
      "outputs": [],
      "source": [
        "print(len(test_dataset))\n",
        "print(len(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall torch-cluster -y\n",
        "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ],
      "metadata": {
        "id": "VanGG-xjofVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch-cluster"
      ],
      "metadata": {
        "id": "G_vpHVAtnn_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk1o-6NFmlqi"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(test_dataset[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGN2W_kBmlqj"
      },
      "outputs": [],
      "source": [
        "loader_type = torch_geometric.loader.DataLoader\n",
        "train_loader = loader_type( train_dataset, shuffle=True, batch_size=128, pin_memory=True, num_workers=0,drop_last=True)\n",
        "val_loader = loader_type( val_dataset, shuffle=True, batch_size=128, pin_memory=True, num_workers=0,drop_last=True)\n",
        "test_loader = loader_type( test_dataset, shuffle=True, batch_size=128, pin_memory=True, num_workers=0,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break"
      ],
      "metadata": {
        "id": "EpFzpWOTZGzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# clearing cuda cache memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "id": "p54DfaGnVfOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgEWkEMHmlqj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleGAT(nn.Module):\n",
        "    def __init__(self,  x_size, edge_feat='none', k=7, use_pe=False, pe_scales=0):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.edge_feat = edge_feat\n",
        "        # self.args = args\n",
        "        if self.edge_feat == 'none':\n",
        "            edge_dim = None\n",
        "\n",
        "        self.gat_conv_1 = torch_geometric.nn.GATv2Conv(\n",
        "            in_channels=x_size if not use_pe else x_size * (pe_scales * 2 + 1),\n",
        "            out_channels=16,\n",
        "            heads=4,\n",
        "            edge_dim=edge_dim\n",
        "        )\n",
        "        self.bn_1 = torch_geometric.nn.BatchNorm(64)\n",
        "        self.gat_conv_2 = torch_geometric.nn.GATv2Conv(\n",
        "            in_channels=16 * 4,\n",
        "            out_channels=32,\n",
        "            heads=4,\n",
        "            edge_dim=edge_dim\n",
        "        )\n",
        "        self.bn_2 = torch_geometric.nn.BatchNorm(128)\n",
        "        self.act = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        pos = data.pos\n",
        "        batch = data.batch\n",
        "        x = data.x\n",
        "\n",
        "        edge_index = torch_geometric.nn.knn_graph(x=pos, k=self.k, batch=batch)\n",
        "        if self.edge_feat == 'none':\n",
        "            edge_attr = None\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Edge feat {self.edge_feat} is not implemented\")\n",
        "\n",
        "        x_out = self.act(self.bn_1(self.gat_conv_1(\n",
        "            x, edge_index, edge_attr=edge_attr)))\n",
        "        x_out = self.act(self.bn_2(self.gat_conv_2(\n",
        "            x_out, edge_index, edge_attr=edge_attr)))\n",
        "\n",
        "        x_out = torch_geometric.nn.global_mean_pool(x_out, batch)\n",
        "\n",
        "        return x_out\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Model prints with number of trainable parameters\n",
        "        \"\"\"\n",
        "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj-JqFSxmlqk"
      },
      "outputs": [],
      "source": [
        "class MLPStack(torch.nn.Module):\n",
        "    def __init__(self, layers, bn=True, act=True, p=0):\n",
        "        super().__init__()\n",
        "        assert len(layers) > 1, \"At least input and output channels must be provided\"\n",
        "\n",
        "        modules = []\n",
        "        for i in range(1, len(layers)):\n",
        "            modules.append(\n",
        "                torch.nn.Linear(layers[i-1], layers[i])\n",
        "            )\n",
        "            modules.append(\n",
        "                torch.nn.BatchNorm1d(layers[i]) if bn == True else torch.nn.Identity()\n",
        "            )\n",
        "            modules.append(\n",
        "                torch.nn.SiLU() if bn == True else torch.nn.Identity()\n",
        "            )\n",
        "            modules.append(\n",
        "                torch.nn.Dropout(p=p) if p != 0 else torch.nn.Identity()\n",
        "            )\n",
        "\n",
        "        self.mlp_stack = torch.nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp_stack(x)\n",
        "\n",
        "class RegressModel(nn.Module):\n",
        "    def __init__(self, model, in_features, predict_bins=True, num_bins=10):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.predict_bins = predict_bins\n",
        "\n",
        "        self.out_mlp = MLPStack(\n",
        "            [in_features + 3, in_features * 2, in_features * 2, in_features, in_features // 2],\n",
        "            bn=True, act=True\n",
        "        )\n",
        "\n",
        "        self.out_regress = torch.nn.Linear(in_features//2, 1)\n",
        "\n",
        "        if self.predict_bins:\n",
        "            self.out_pred = torch.nn.Linear(in_features // 2, num_bins)\n",
        "\n",
        "    def forward(self, data):\n",
        "        return_dict = {}\n",
        "\n",
        "        print(data[0].x,data[0].pos,data[0].pt,data[0].y,data[0].ieta)\n",
        "        out = self.model(data)\n",
        "        out = torch.cat(\n",
        "            [out, data.pt.unsqueeze(-1), data.ieta.unsqueeze(-1), data.iphi.unsqueeze(-1)], dim=1\n",
        "        )\n",
        "        out = self.out_mlp(out)\n",
        "        regress_out = self.out_regress(out)\n",
        "\n",
        "        return_dict['regress'] = regress_out\n",
        "\n",
        "        if self.predict_bins:\n",
        "            pred_out = self.out_pred(out)\n",
        "            return_dict['class'] = pred_out\n",
        "\n",
        "        return return_dict\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Model prints with number of trainable parameters\n",
        "        \"\"\"\n",
        "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
        "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "        return super().__str__() + '\\nTrainable parameters: {}'.format(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_K6G-Q3mlqk"
      },
      "outputs": [],
      "source": [
        "input_size = 30\n",
        "\n",
        "\n",
        "model = SimpleGAT(x_size=input_size)\n",
        "Final_model=RegressModel(model, in_features = 128, predict_bins=True, num_bins=10)\n",
        "Final_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaJmxYRZmlql"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming Final_model, train_loader, and val_loader are defined\n",
        "lr = 0.001  # Assuming lr is defined\n",
        "\n",
        "criterion = torch.nn.MSELoss().to(device)\n",
        "trainable_params = filter(lambda p: p.requires_grad, Final_model.parameters())\n",
        "# # Convert the filter object to a list or another iterable type\n",
        "# trainable_params_list = list(trainable_params)\n",
        "\n",
        "# # Print the trainable parameters\n",
        "# print(trainable_params_list)\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
        "\n",
        "criterion_dict = {}\n",
        "criterion_dict['regress'] = torch.nn.MSELoss()\n",
        "#criterion_dict['class'] = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(2):\n",
        "    # Training\n",
        "    Final_model.train()\n",
        "    tqdm_iter = tqdm(train_loader, total=len(train_loader))\n",
        "    tqdm_iter.set_description(f\"Epoch {epoch} - Training\")\n",
        "\n",
        "    total_train_loss = 0.0\n",
        "    true_preds, num_preds = 0., 0.\n",
        "    for it, batch in enumerate(tqdm_iter):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        m = batch.y\n",
        "        out = Final_model(batch)\n",
        "\n",
        "        loss_dict = {}\n",
        "        loss = 0\n",
        "\n",
        "        for i in range(len(batch)):\n",
        "                #print(\"out\",out['regress'][i],\"batch_yclass\",batch.y_class[i])\n",
        "            if(torch.floor(10*out['regress'][i])==batch.y_class[i]):\n",
        "                true_preds+=1\n",
        "                #print(\"out\",torch.floor(10*out['regress'][i])==batch.y_class[i])\n",
        "            #print(torch.floor(out['regress']*10))\n",
        "        num_preds += len(batch)\n",
        "        m = m * m1_scale + m0_scale\n",
        "        out['regress'] = out['regress'] * m1_scale + m0_scale\n",
        "        loss_dict['regress'] = criterion(out['regress'], m.unsqueeze(-1))\n",
        "        loss += loss_dict['regress']\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch} - Average Training Loss: {average_train_loss:.4f}\")\n",
        "    print(\"train acc\",true_preds/num_preds)\n",
        "    # Validation\n",
        "    Final_model.eval()\n",
        "    val_tqdm_iter = tqdm(val_loader, total=len(val_loader))\n",
        "    val_tqdm_iter.set_description(f\"Epoch {epoch} - Validation\")\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    true_preds, num_preds = 0., 0.\n",
        "    for it, batch in enumerate(val_tqdm_iter):\n",
        "        with torch.no_grad():\n",
        "            batch = batch.to(device)\n",
        "            m = batch.y\n",
        "            out = Final_model(batch)\n",
        "\n",
        "            loss_dict = {}\n",
        "            loss = 0\n",
        "\n",
        "            for i in range(len(batch)):\n",
        "                #print(\"out\",out['regress'][i],\"batch_yclass\",batch.y_class[i])\n",
        "                if(torch.floor(10*out['regress'][i])==batch.y_class[i]):\n",
        "                    true_preds+=1\n",
        "                #print(\"out\",torch.floor(10*out['regress'][i])==batch.y_class[i])\n",
        "            #print(torch.floor(out['regress']*10))\n",
        "            num_preds += len(batch)\n",
        "\n",
        "\n",
        "            # Scale both target and predicted values before calculating the loss\n",
        "            m = m * m1_scale + m0_scale\n",
        "            out['regress'] = out['regress'] * m1_scale + m0_scale\n",
        "\n",
        "# Calculate the loss on scaled values\n",
        "            loss_dict['regress'] = criterion(out['regress'], m.unsqueeze(-1))\n",
        "            loss += loss_dict['regress']\n",
        "\n",
        "\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch} - Average Validation Loss: {average_val_loss:.4f}\")\n",
        "    print(\"Val acc\",true_preds/num_preds)\n",
        "    # Print or compute validation accuracy if applicable\n",
        "    # Add your validation accuracy computation code here\n",
        "\n",
        "    # Reset the model back to training mode\n",
        "    Final_model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "    Final_model.eval()\n",
        "    val_tqdm_iter = tqdm(val_loader, total=len(val_loader))\n",
        "    val_tqdm_iter.set_description(f\"Epoch {epoch} - Validation\")\n",
        "\n",
        "    total_val_loss = 0.0\n",
        "    true_preds, num_preds = 0., 0.\n",
        "    for it, batch in enumerate(val_tqdm_iter):\n",
        "        with torch.no_grad():\n",
        "            batch = batch.to(device)\n",
        "            m = batch.y\n",
        "            out = Final_model(batch)\n",
        "\n",
        "            loss_dict = {}\n",
        "            loss = 0\n",
        "\n",
        "            for i in range(len(batch)):\n",
        "                print(\"out\",out['regress'][i],\"batch_yclass\",batch.y_class[i])\n",
        "                if(torch.floor(10*out['regress'][i])==batch.y_class[i]):\n",
        "                    true_preds+=1\n",
        "                #print(\"out\",torch.floor(10*out['regress'][i])==batch.y_class[i])\n",
        "            #print(torch.floor(out['regress']*10))\n",
        "            num_preds += len(batch)\n",
        "\n",
        "\n",
        "            # Scale both target and predicted values before calculating the loss\n",
        "            m = m * m1_scale + m0_scale\n",
        "            out['regress'] = out['regress'] * m1_scale + m0_scale\n",
        "\n",
        "# Calculate the loss on scaled values\n",
        "            loss_dict['regress'] = criterion(out['regress'], m.unsqueeze(-1))\n",
        "            loss += loss_dict['regress']\n",
        "\n",
        "\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch} - Average Validation Loss: {average_val_loss:.4f}\")\n",
        "    print(\"Val acc\",true_preds/num_preds)"
      ],
      "metadata": {
        "id": "-fyK88cCa9DB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}