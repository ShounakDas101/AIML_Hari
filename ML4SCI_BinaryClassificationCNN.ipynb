{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShounakDas101/AIML_Hari/blob/main/ML4SCI_BinaryClassificationCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-uAubij3lQL",
        "outputId": "696c420d-085e-47c8-8b33-0c7f3af5d843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import h5py\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9YDMhSP3lQP"
      },
      "outputs": [],
      "source": [
        "DATA_DIR='./data'\n",
        "'''\n",
        "IMAGE_SIZE = 32 # W=32 H=32\n",
        "NUM_CHANNELS = 2 # R G B then it should be 3\n",
        "PATCH_SIZE = 4 # W_patch=4 H_patch=4\n",
        "NUM_PATCHES = (IMAGE_SIZE//PATCH_SIZE)*(IMAGE_SIZE//PATCH_SIZE) # = 64\n",
        "NUM_HEADS = 8  # enbedding dim should be divisible by no of heads\n",
        "EMBBEDING_DIM = PATCH_SIZE*PATCH_SIZE*NUM_CHANNELS  #NUM_PATCHNELS\n",
        "MLP_SIZE = 128\n",
        "NUM_LAYERS = 8\n",
        "'''\n",
        "NUM_CLASSES = 2\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 40\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRJEqJ1M3lQR",
        "outputId": "eb4a1b18-b65c-4962-e0c2-3068657d9e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# clearing cuda cache memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j43bn9E3lQR",
        "outputId": "0e3c8443-7d2a-42d7-db2b-dcfbd25d2d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The file 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5' exists.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Specify the path to the file in your Google Drive\n",
        "file_path = 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "# CHECKPOINT_PATH = '/content/drive/MyDrive/CHECKPOINTS'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"The file '{file_path}' exists.\")\n",
        "else:\n",
        "    print(f\"The file '{file_path}' does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZlVIXCl3lQR",
        "outputId": "457da457-22b9-4ad2-83e7-c8db1168ea4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "//X\n",
            "//y\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "\n",
        "def find_keys(hdf5_obj, path='/'):\n",
        "    \"\"\"Recursively find keys in an HDF5 file.\"\"\"\n",
        "    keys = []\n",
        "    for key in hdf5_obj[path].keys():\n",
        "        full_path = f\"{path}/{key}\"\n",
        "        keys.append(full_path)\n",
        "        if isinstance(hdf5_obj[full_path], h5py.Group):\n",
        "            keys.extend(find_keys(hdf5_obj, full_path))\n",
        "    return keys\n",
        "\n",
        "# Open the HDF5 file\n",
        "file_path = 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'  # Replace with your actual file path\n",
        "with h5py.File(file_path, 'r') as hdf5_file:\n",
        "    # Find keys starting from the root\n",
        "    all_keys = find_keys(hdf5_file)\n",
        "\n",
        "# Print the found keys\n",
        "for key in all_keys:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7J90QCg3lQS"
      },
      "outputs": [],
      "source": [
        "# import dataset\n",
        "\n",
        "# importing electron dataset and seperating images and labels\n",
        "electron_dataset = h5py.File(\"SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")\n",
        "electron_imgs=np.array(electron_dataset[\"X\"])\n",
        "electron_labels=np.array(electron_dataset[\"y\"],dtype=np.int64)\n",
        "\n",
        "# importing photon dataset and seperating images and labels\n",
        "photon_dataset = h5py.File(\"SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\",\"r\")\n",
        "photon_imgs=np.array(photon_dataset[\"X\"])\n",
        "photon_labels=np.array(photon_dataset[\"y\"],dtype=np.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QdKKIb33lQS",
        "outputId": "2eb72519-4396-4b8b-bee9-3be1feb9c7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(249000, 32, 32, 2)\n",
            "float32\n",
            "[1 1 1 ... 1 1 1]\n",
            "(249000, 32, 32, 2)\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(electron_imgs.shape)\n",
        "print(electron_imgs.dtype)\n",
        "print(electron_labels)\n",
        "print(photon_imgs.shape)\n",
        "print(photon_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUiVp5y43lQS",
        "outputId": "09e04eb5-5b4f-4d60-99ff-e8ce6688e8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([498000, 32, 32, 2])\n",
            "torch.Size([498000, 2, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "# concatenate electron and photon images/labels\n",
        "img_arrs = torch.Tensor(np.vstack((photon_imgs,electron_imgs)))\n",
        "labels = torch.Tensor(np.hstack((photon_labels,electron_labels))).to(torch.int64)\n",
        "print(img_arrs.shape)\n",
        "img_arrs = img_arrs.permute(0,3,1,2)\n",
        "print(img_arrs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-jxfrvQ3lQT"
      },
      "outputs": [],
      "source": [
        "del electron_imgs,photon_imgs,electron_labels,photon_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "299XV0eD3lQU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data[:,:,4:-4,4:-4]\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKA3meLb3lQU",
        "outputId": "ab89bfec-bd73-44ba-87a9-83255bf8dcc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498000\n"
          ]
        }
      ],
      "source": [
        "# Assuming img_arrs and labels are already defined\n",
        "custom_dataset = CustomDataset(img_arrs, labels)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.7 * len(custom_dataset))\n",
        "valid_size = int(0.2 * len(custom_dataset))\n",
        "test_size = len(custom_dataset) - train_size - valid_size\n",
        "train_dataset, valid_dataset, test_dataset = random_split(custom_dataset, [train_size, valid_size, test_size])\n",
        "print(len(valid_dataset)+len(test_dataset)+len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hqykOoe3lQV"
      },
      "source": [
        "transforms should be used here before data loader generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EbNygar3lQV"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                          num_workers=NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTElsMo83lQW"
      },
      "source": [
        "\n",
        "\n",
        "Step 1 Tasks:\n",
        "\n",
        "1- turn an image into patches\n",
        "\n",
        "2- flatten the patch feature maps into a single dimension\n",
        "\n",
        "3- Convert the output into Desried output (flattened 2D patches): (196, 768) -> N×(P2⋅C) #Current shape: (1, 768, 196)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMh4Yode3lQW",
        "outputId": "09cbcf48-0a2d-4eb6-e7c0-bb43ba98dd58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data inputs torch.Size([256, 2, 24, 24]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs = next(iter(train_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs['image'].shape, \"\\n\")\n",
        "# print(\"Full\",data_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTIjlv-C3lQW",
        "outputId": "5191aef0-a144-42ac-ddaf-8f4302c7ac4f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(data_inputs['image'][0,1:2,:,:].permute(1,2,0))  # Assuming it's a 2-channel image; adjust if neededcmap='gray'\n",
        "plt.title('Electron Image ch1')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjLSqMxB3lQW",
        "outputId": "cde80806-d622-4264-c8b4-657d930fecde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 32, 24, 24])\n",
            "256 18432\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-5.5154e-02, -1.2803e-01],\n",
              "        [-5.2103e-02, -1.3500e-01],\n",
              "        [ 7.2689e-03, -1.3097e-01],\n",
              "        [-6.0871e-02, -1.3446e-01],\n",
              "        [-3.1944e-02, -1.3867e-01],\n",
              "        [-1.7648e-02, -1.1536e-01],\n",
              "        [-4.8125e-02, -1.2411e-01],\n",
              "        [-5.9432e-02, -1.0735e-01],\n",
              "        [ 1.6285e-03, -1.7235e-01],\n",
              "        [-5.8095e-03, -1.3022e-01],\n",
              "        [-5.8623e-02, -1.2938e-01],\n",
              "        [-5.3715e-02, -1.1010e-01],\n",
              "        [-4.5587e-02, -1.1092e-01],\n",
              "        [-2.6330e-02, -1.2500e-01],\n",
              "        [-5.6937e-02, -1.3133e-01],\n",
              "        [-9.0975e-02, -1.2946e-01],\n",
              "        [-4.6858e-02, -1.2257e-01],\n",
              "        [-5.5542e-02, -1.2132e-01],\n",
              "        [-2.7580e-02, -1.3476e-01],\n",
              "        [-2.0765e-02, -1.1341e-01],\n",
              "        [-5.0439e-02, -1.1576e-01],\n",
              "        [-4.7667e-02, -1.4551e-01],\n",
              "        [-3.7667e-02, -1.1841e-01],\n",
              "        [-3.0951e-02, -1.3134e-01],\n",
              "        [-4.6983e-02, -1.2617e-01],\n",
              "        [-2.6739e-02, -1.4490e-01],\n",
              "        [-3.6488e-02, -1.1606e-01],\n",
              "        [-1.9706e-02, -1.0628e-01],\n",
              "        [-6.9106e-02, -1.0864e-01],\n",
              "        [-5.2884e-02, -1.4405e-01],\n",
              "        [-7.4854e-02, -1.4973e-01],\n",
              "        [-6.9754e-02, -1.4071e-01],\n",
              "        [-5.2441e-02, -9.2592e-02],\n",
              "        [-3.0817e-02, -1.4258e-01],\n",
              "        [-1.7690e-02, -1.3260e-01],\n",
              "        [-4.1709e-02, -1.1173e-01],\n",
              "        [-5.3055e-02, -1.2880e-01],\n",
              "        [-4.8275e-02, -1.4079e-01],\n",
              "        [-2.3746e-02, -1.1536e-01],\n",
              "        [-3.1301e-02, -1.2419e-01],\n",
              "        [-7.2015e-02, -1.4132e-01],\n",
              "        [-6.3703e-02, -1.3909e-01],\n",
              "        [-4.7592e-02, -1.0901e-01],\n",
              "        [-1.1276e-02, -1.3044e-01],\n",
              "        [-6.0115e-02, -1.0557e-01],\n",
              "        [-5.2141e-02, -1.2251e-01],\n",
              "        [-3.8253e-02, -1.2966e-01],\n",
              "        [-2.2002e-02, -1.5527e-01],\n",
              "        [-4.4218e-04, -1.8408e-01],\n",
              "        [-4.1039e-02, -1.1717e-01],\n",
              "        [-7.5109e-02, -1.0131e-01],\n",
              "        [-4.5649e-02, -1.4199e-01],\n",
              "        [-4.3783e-02, -1.8979e-01],\n",
              "        [-3.1208e-02, -1.3664e-01],\n",
              "        [-3.2036e-02, -1.3500e-01],\n",
              "        [-3.0661e-02, -1.2686e-01],\n",
              "        [-6.8085e-02, -1.0392e-01],\n",
              "        [-5.8016e-02, -1.1139e-01],\n",
              "        [-1.2712e-02, -1.6172e-01],\n",
              "        [-5.0054e-02, -1.1330e-01],\n",
              "        [-3.3675e-02, -1.1775e-01],\n",
              "        [-1.6330e-02, -1.2933e-01],\n",
              "        [-4.2820e-02, -9.7266e-02],\n",
              "        [-2.2237e-02, -1.0389e-01],\n",
              "        [-8.1064e-02, -1.3309e-01],\n",
              "        [-6.3961e-02, -1.8698e-01],\n",
              "        [-3.7728e-02, -1.7029e-01],\n",
              "        [-3.4227e-02, -1.3072e-01],\n",
              "        [-8.8204e-02, -1.1656e-01],\n",
              "        [-7.5828e-02, -1.1947e-01],\n",
              "        [-6.4993e-02, -1.2722e-01],\n",
              "        [-4.7792e-03, -1.4034e-01],\n",
              "        [-1.0800e-02, -1.2901e-01],\n",
              "        [-5.1236e-02, -9.4845e-02],\n",
              "        [-6.4239e-02, -1.2054e-01],\n",
              "        [-1.7832e-02, -1.2748e-01],\n",
              "        [-4.9364e-02, -1.4261e-01],\n",
              "        [-4.5780e-02, -1.7780e-01],\n",
              "        [-5.9427e-02, -1.2542e-01],\n",
              "        [-9.3846e-03, -1.5929e-01],\n",
              "        [-1.9564e-02, -1.2271e-01],\n",
              "        [-6.0984e-02, -1.2561e-01],\n",
              "        [-3.9559e-02, -1.2617e-01],\n",
              "        [-2.0886e-02, -1.1737e-01],\n",
              "        [-3.2298e-02, -2.0037e-01],\n",
              "        [-1.0398e-02, -9.2715e-02],\n",
              "        [-3.8079e-02, -1.1782e-01],\n",
              "        [-5.0226e-02, -1.2399e-01],\n",
              "        [-2.0984e-02, -1.3558e-01],\n",
              "        [-5.9486e-02, -1.7290e-01],\n",
              "        [-3.3358e-02, -1.0657e-01],\n",
              "        [-6.6002e-02, -1.1330e-01],\n",
              "        [ 1.2382e-04, -1.7139e-01],\n",
              "        [-1.7782e-02, -1.4329e-01],\n",
              "        [-4.5120e-02, -1.1684e-01],\n",
              "        [-2.8156e-02, -1.2794e-01],\n",
              "        [-7.6230e-02, -9.9961e-02],\n",
              "        [-7.2337e-02, -1.1808e-01],\n",
              "        [-6.3117e-03, -1.3044e-01],\n",
              "        [-3.6946e-02, -1.3071e-01],\n",
              "        [-5.1221e-02, -1.1076e-01],\n",
              "        [-6.4857e-02, -1.5547e-01],\n",
              "        [-5.1094e-02, -1.1555e-01],\n",
              "        [-2.8205e-02, -1.1131e-01],\n",
              "        [-5.4659e-02, -1.4789e-01],\n",
              "        [-5.8064e-02, -1.0489e-01],\n",
              "        [-2.0049e-02, -1.0243e-01],\n",
              "        [-2.5685e-02, -1.4458e-01],\n",
              "        [-3.9874e-02, -1.0086e-01],\n",
              "        [-5.3186e-02, -1.2876e-01],\n",
              "        [-4.9753e-02, -1.2665e-01],\n",
              "        [-3.2597e-02, -1.1706e-01],\n",
              "        [-3.0979e-02, -1.5758e-01],\n",
              "        [-3.3384e-02, -1.1914e-01],\n",
              "        [ 1.7223e-03, -1.5823e-01],\n",
              "        [-3.5332e-02, -1.5624e-01],\n",
              "        [-3.2479e-02, -1.4623e-01],\n",
              "        [-3.7942e-02, -1.4022e-01],\n",
              "        [-3.7719e-02, -1.3015e-01],\n",
              "        [-4.4723e-02, -1.1818e-01],\n",
              "        [-3.6655e-02, -1.2088e-01],\n",
              "        [-5.8206e-02, -1.2990e-01],\n",
              "        [-4.9569e-02, -1.0510e-01],\n",
              "        [-4.7582e-02, -1.1600e-01],\n",
              "        [-5.1780e-02, -1.4277e-01],\n",
              "        [-2.8231e-02, -1.3359e-01],\n",
              "        [-5.9591e-02, -1.5390e-01],\n",
              "        [-2.5363e-02, -1.1959e-01],\n",
              "        [-6.2273e-02, -1.4936e-01],\n",
              "        [-5.0946e-02, -1.1941e-01],\n",
              "        [-5.7881e-02, -1.1856e-01],\n",
              "        [-4.3992e-02, -1.2716e-01],\n",
              "        [-8.5637e-02, -1.3474e-01],\n",
              "        [-4.9347e-02, -9.5097e-02],\n",
              "        [-6.8895e-02, -1.1432e-01],\n",
              "        [-4.6184e-02, -1.3923e-01],\n",
              "        [-5.2976e-02, -1.5875e-01],\n",
              "        [-3.3531e-02, -1.5065e-01],\n",
              "        [ 4.4884e-03, -1.4657e-01],\n",
              "        [-7.4210e-02, -1.2330e-01],\n",
              "        [-3.1647e-02, -1.6456e-01],\n",
              "        [-6.9100e-02, -1.3298e-01],\n",
              "        [-2.1302e-02, -1.4215e-01],\n",
              "        [-6.7294e-02, -1.0059e-01],\n",
              "        [-8.5929e-02, -6.8226e-02],\n",
              "        [-2.2078e-02, -1.3215e-01],\n",
              "        [-3.6762e-02, -1.0755e-01],\n",
              "        [-2.3572e-02, -1.4689e-01],\n",
              "        [-2.0894e-02, -1.6524e-01],\n",
              "        [-5.8586e-02, -1.3903e-01],\n",
              "        [-3.4478e-02, -1.3177e-01],\n",
              "        [-5.3311e-02, -1.6585e-01],\n",
              "        [-3.2381e-02, -1.2550e-01],\n",
              "        [-4.7518e-02, -1.3132e-01],\n",
              "        [-8.1956e-02, -1.3573e-01],\n",
              "        [-5.7495e-02, -1.1935e-01],\n",
              "        [-4.3383e-02, -1.6930e-01],\n",
              "        [-4.6563e-02, -1.1107e-01],\n",
              "        [-4.0104e-02, -1.0569e-01],\n",
              "        [-4.7797e-03, -1.5051e-01],\n",
              "        [-3.9565e-02, -1.1800e-01],\n",
              "        [-8.4260e-03, -9.7891e-02],\n",
              "        [-2.9063e-02, -1.1669e-01],\n",
              "        [-5.1455e-02, -1.3008e-01],\n",
              "        [-8.6976e-02, -1.7044e-01],\n",
              "        [-8.2735e-02, -1.2357e-01],\n",
              "        [-3.4942e-02, -1.5325e-01],\n",
              "        [-3.5700e-02, -1.1927e-01],\n",
              "        [-5.1467e-02, -1.3381e-01],\n",
              "        [-5.4848e-02, -1.2419e-01],\n",
              "        [ 8.4774e-03, -1.0551e-01],\n",
              "        [-1.0581e-02, -9.7973e-02],\n",
              "        [-4.9943e-02, -1.4491e-01],\n",
              "        [-3.9873e-02, -1.6873e-01],\n",
              "        [-5.6113e-02, -1.1165e-01],\n",
              "        [-4.9907e-02, -1.1736e-01],\n",
              "        [-6.4020e-02, -1.3148e-01],\n",
              "        [-2.2350e-02, -1.4186e-01],\n",
              "        [-8.5043e-02, -1.3542e-01],\n",
              "        [-5.4517e-02, -1.1618e-01],\n",
              "        [-6.0324e-02, -1.6274e-01],\n",
              "        [-3.4554e-02, -1.4232e-01],\n",
              "        [-2.4764e-02, -1.1955e-01],\n",
              "        [-3.6923e-02, -7.9834e-02],\n",
              "        [-8.1124e-02, -1.1322e-01],\n",
              "        [-6.5383e-02, -1.8477e-01],\n",
              "        [-5.8784e-02, -1.0644e-01],\n",
              "        [-6.4739e-02, -1.4520e-01],\n",
              "        [-5.4619e-02, -1.2105e-01],\n",
              "        [-2.9639e-02, -1.1160e-01],\n",
              "        [-7.6315e-02, -1.4178e-01],\n",
              "        [-5.4669e-02, -1.0968e-01],\n",
              "        [-6.1427e-02, -1.3180e-01],\n",
              "        [-6.3792e-02, -1.2797e-01],\n",
              "        [-7.7618e-02, -1.4872e-01],\n",
              "        [-3.9922e-02, -1.4293e-01],\n",
              "        [-4.2849e-02, -1.5255e-01],\n",
              "        [-6.2291e-02, -1.4932e-01],\n",
              "        [-6.4365e-02, -1.3131e-01],\n",
              "        [-7.2370e-02, -1.3307e-01],\n",
              "        [-6.6195e-02, -1.2372e-01],\n",
              "        [-2.2899e-02, -1.3760e-01],\n",
              "        [-4.0985e-02, -1.2054e-01],\n",
              "        [-4.7785e-02, -1.3672e-01],\n",
              "        [-1.4581e-02, -1.3752e-01],\n",
              "        [-4.2108e-02, -1.0796e-01],\n",
              "        [-6.0600e-02, -1.2486e-01],\n",
              "        [-4.9940e-02, -1.2814e-01],\n",
              "        [-3.5716e-02, -8.8406e-02],\n",
              "        [-4.9365e-02, -1.1974e-01],\n",
              "        [-1.5590e-02, -1.2853e-01],\n",
              "        [-3.3118e-02, -1.2396e-01],\n",
              "        [-6.9152e-02, -1.2955e-01],\n",
              "        [-7.9346e-02, -1.2562e-01],\n",
              "        [-5.5059e-02, -1.2617e-01],\n",
              "        [-3.4229e-02, -1.5394e-01],\n",
              "        [-4.6796e-02, -1.0950e-01],\n",
              "        [ 2.7299e-02, -7.8963e-02],\n",
              "        [-2.9367e-02, -1.3513e-01],\n",
              "        [-4.7157e-02, -1.1572e-01],\n",
              "        [-4.8771e-02, -1.3171e-01],\n",
              "        [-3.0420e-02, -1.1046e-01],\n",
              "        [-5.4570e-02, -1.0979e-01],\n",
              "        [-1.4337e-02, -4.9714e-02],\n",
              "        [-6.2177e-02, -1.2740e-01],\n",
              "        [-1.7672e-02, -1.1863e-01],\n",
              "        [-3.4516e-02, -1.3930e-01],\n",
              "        [-5.4949e-02, -1.2811e-01],\n",
              "        [-6.1130e-02, -1.1302e-01],\n",
              "        [-5.3051e-02, -1.2161e-01],\n",
              "        [-3.6696e-02, -1.6358e-01],\n",
              "        [-4.7395e-02, -1.4630e-01],\n",
              "        [-5.5502e-02, -1.4918e-01],\n",
              "        [-2.3949e-02, -1.2057e-01],\n",
              "        [-4.7251e-02, -9.5498e-02],\n",
              "        [-5.0982e-02, -9.5056e-02],\n",
              "        [-5.0347e-02, -1.0927e-01],\n",
              "        [-2.7926e-02, -1.5957e-01],\n",
              "        [-5.2980e-02, -1.3861e-01],\n",
              "        [-2.8701e-02, -1.1286e-01],\n",
              "        [-2.1264e-02, -1.3051e-01],\n",
              "        [-2.1942e-02, -1.5301e-01],\n",
              "        [-2.9947e-02, -1.4591e-01],\n",
              "        [-3.8398e-02, -1.0520e-01],\n",
              "        [-6.1371e-02, -1.2990e-01],\n",
              "        [-5.8240e-02, -1.3912e-01],\n",
              "        [-3.3004e-02, -1.1458e-01],\n",
              "        [-1.5009e-02, -1.5009e-01],\n",
              "        [-3.0094e-02, -1.0842e-01],\n",
              "        [-5.7135e-02, -1.2336e-01],\n",
              "        [-1.9522e-02, -1.3289e-01],\n",
              "        [-1.1588e-01, -1.5667e-01],\n",
              "        [-4.6556e-02, -1.1256e-01],\n",
              "        [-3.2468e-02, -1.2093e-01],\n",
              "        [-7.0101e-02, -1.1983e-01],\n",
              "        [-3.8389e-02, -1.2992e-01]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_conv1 = nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, padding=1)\n",
        "output=my_conv1((data_inputs['image']))\n",
        "print(output.shape)\n",
        "relu = nn.ReLU()\n",
        "output=output.view(BATCH_SIZE,-1)\n",
        "print(output.shape[0],output.shape[1])\n",
        "fc = nn.Linear(output.shape[1],2)\n",
        "fc(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_rh9MBO3lQX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1,dropout_rate=.2):\n",
        "        super().__init__()\n",
        "        self.my_conv1 = nn.Conv2d(in_channels=2, out_channels=8, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.Linear1=nn.Linear(24*24*8,24*24*1)\n",
        "        self.Linear2=nn.Linear(24*24*1,24*8)\n",
        "        self.Linear3=nn.Linear(24*8,4)\n",
        "\n",
        "        self.Linear=nn.Linear(4,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x['image'] =self.my_conv1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        #x['image'] =self.my_conv1(x['image'])\n",
        "        #x['image'] =self.relu(x['image'])\n",
        "        x['image'] =x['image'].view(BATCH_SIZE,-1)\n",
        "\n",
        "        x['image'] =self.Linear1(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear2(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear3(x['image'])\n",
        "        x['image'] =self.relu(x['image'])\n",
        "        x['image'] =self.dropout(x['image'])\n",
        "        x['image'] =self.Linear(x['image'])\n",
        "        return x['image']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx7w7p-f3lQX",
        "outputId": "6579c30b-95c6-499b-bf1a-871490b3be09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyModel(\n",
            "  (my_conv1): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (Linear1): Linear(in_features=4608, out_features=576, bias=True)\n",
            "  (Linear2): Linear(in_features=576, out_features=48, bias=True)\n",
            "  (Linear3): Linear(in_features=48, out_features=4, bias=True)\n",
            "  (Linear): Linear(in_features=4, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = MyModel(num_classes=1)\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQwwFKOw3lQX"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQzv9fML3lQX"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "loss_module = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "def train_model(model, optimizer, data_loader, val_loader,loss_module, num_epochs=EPOCHS):\n",
        "    train_losses = []  # Changed variable name from train_loss to train_losses\n",
        "    val_losses = []\n",
        "\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs in data_loader:\n",
        "            # Check if the current batch is the last one\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * BATCH_SIZE # batch_size\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        #validation plase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "          for data_inputs in val_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "\n",
        "            # Ensure that the predictions have the same data type as the labels\n",
        "            preds = preds.squeeze(dim=1).to(data_inputs['label'].float())\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_inputs['label'].float())\n",
        "            running_loss += loss.item() * BATCH_SIZE\n",
        "        val_loss = running_loss / len(val_loader.dataset)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss {train_loss}, Validation Loss {val_loss}\")\n",
        "        eval_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGKuOyy43lQY",
        "outputId": "af0eb2fc-8a1b-4e47-e53c-7cc5747e0812"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40 - Train loss 0.6927884005679527, Validation Loss 0.6927721600250751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▎         | 1/40 [00:09<06:06,  9.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 50.05%\n",
            "Epoch 2/40 - Train loss 1.3855432564433936, Validation Loss 0.6927690242257395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:18<05:53,  9.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model: 50.05%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:26<08:28, 13.38s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(model, optimizer, train_loader, val_loader,loss_module)\n",
            "\u001b[1;32m/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m preds \u001b[39m=\u001b[39m model(data_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Ensure that the predictions have the same data type as the labels\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m preds \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39;49msqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(data_inputs[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m## Step 3: Calculate the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hari/MY_AIML/ML4SCI/ML4SCI_BinaryClassificationCNN.ipynb#X30sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_module(preds, data_inputs[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat())\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model, optimizer, train_loader, val_loader,loss_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv9XLWIC3lQY"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs in data_loader:\n",
        "            is_last_batch = data_inputs['image'].shape[0] < BATCH_SIZE\n",
        "            if is_last_batch:\n",
        "                break\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs['image'] = data_inputs['image'].to(device)\n",
        "            data_inputs['label'] = data_inputs['label'].to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_inputs['label']).sum()\n",
        "            num_preds += BATCH_SIZE\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    return acc\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3e1Gjsf3lQY",
        "outputId": "46342b47-a1be-452e-f38b-11a832a034d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7020, device='cuda:0')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#eval_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}